The Big Idea: Understanding the Project's Goal

At its heart, this project is an experiment to prove one thing: AI agents get smarter and more reliable if you "compile" their prompts instead of just writing them by hand.

The Problem: When you give an LLM a handwritten prompt (e.g., "Rate how important this is"), it works okay, but it might be inconsistent.

The DSPy Solution: DSPy treats that prompt like a program. You provide a "training set" of examples (called seeds), and an optimizer (like GEPA) automatically refines the prompt to be much more effective and reliable for your specific task.

The Mini-Town: The town simulation with 5 agents is just a fun and visual test environment (the document calls it "scaffolding"). It's a way to measure whether the "compiled" agents actually behave better than the "uncompiled" ones in a complex, interactive world.

Think of it like this: you're not building a game. You're building a science experiment, and the game-like town is your laboratory.

What to Look For After Each Milestone

Here is your step-by-step guide to implementation, focusing on the goal of each phase and the tangible output you should have.

Day 0.5: Hardcoded Validation

ðŸŽ¯ Goal: Build the absolute barebones skeleton of your application. The goal is to prove that the frontend, backend, and database can talk to each other without any AI complexity. This is the most critical step for avoiding future headaches.

âœ… What "Done" Looks Like:

You can run the backend server (uvicorn main:app).

You can run the frontend app (npm run dev).

A browser window opens showing a simple map with 3-5 circles or sprites on it.

The circles move around randomly.

The backend is successfully sending position updates to the frontend over a WebSocket (the circles' movement is smooth).

When two circles get very close, a message prints in your backend terminal console, like INFO: Agent 'alice' perceived Agent 'bob'.

A town.db file has been created in your data/ folder.

ðŸ¤” Why It Matters: If you can't get this simple loop working, adding slow, complex LLM calls will be impossible to debug. This step confirms your basic architecture is sound.

Day 1: DuckDB + Vector Setup

ðŸŽ¯ Goal: Give your agents a functioning "memory." This involves setting up the database to not only store text but also to search it by meaning (vector search).

âœ… What "Done" Looks Like:

You have a Python script (memory.py) with functions like store_memory() and retrieve_memories().

When you call store_memory(), a new row is added to the memories table in town.db, and the embedding column is filled with a list of 384 numbers.

You can run a test query like retrieve_memories(agent_id='alice', query='party invitation').

The result should include memories with content like "Maria is having a get-together" even if the word "party" isn't explicitly there. This proves your vector search is working.

ðŸ¤” Why It Matters: An agent without memory is just a reactive bot. This step builds the foundation for all learning, reflection, and planning.

Day 2: Latency Baseline + Uncompiled DSPy

ðŸŽ¯ Goal: Hook up the real AI (LLM) for the first time and, most importantly, measure how slow it is. You'll use the basic, uncompiled DSPy modules.

âœ… What "Done" Looks Like:

Your agents are now making calls to the Groq API. You can see this in your server logs.

You've implemented the timed_llm_call wrapper. Your logs now contain messages like [ScoreImportance] call took 0.62s.

After running the simulation for 5-10 minutes, you can look at your latency_tracker data. You should be able to state a clear number: "The p95 latency for ScoreImportance is 1.4 seconds."

Based on this number, you make your first critical decision: "Since 1.4s is less than my 2s tick interval, I can keep it. If it were higher, I'd need to increase the tick interval to 3s."

ðŸ¤” Why It Matters: If your agent's brain (LLM) takes longer to think than the world takes to update (tick), the simulation will lag and break. This step establishes your simulation's "speed limit."

Day 3: Seed Collection

ðŸŽ¯ Goal: Create the high-quality "answer key" that the DSPy optimizer will use to learn. This is a manual, human-intensive task that is make-or-break for the entire project.

âœ… What "Done" Looks Like:

You have a JSON file, seeds/scorer_v1.json, with 30-40 examples. Each example has an observation, a gold-standard score, and a rationale explaining why it got that score.

The examples cover a wide range of situations (social, mundane, emergency).

You have proof that your scoring rules are consistent. You've either had a friend rate a few examples and confirmed you agree, or you've calculated the Cohen's kappa score and it's above 0.6.

You have a rationale_guide.md file that clearly explains your scoring system.

ðŸ¤” Why It Matters: Garbage In, Garbage Out. If your seeds are low-quality, ambiguous, or biased, the multi-hour compilation process on Day 4 will be wasted, and the resulting "compiled" agent will be no better (or even worse) than the original.

Day 4: Compilation

ðŸŽ¯ Goal: Run the DSPy optimizer to automatically generate a superior prompt for your ScoreImportance module.

âœ… What "Done" Looks Like:

A Colab notebook successfully runs from start to finish without errors.

It loads your scorer_v1.json seed file.

It runs the GEPA optimizer. This will take several hours and you'll see lots of output.

At the end, a new file, compiled/compiled_scorer.json, has been created and saved (e.g., to your Google Drive). This file contains the optimized prompt program.

ðŸ¤” Why It Matters: This is the core action of the project. You are transforming your human knowledge (the seeds) into a highly-optimized program that the LLM can run more effectively.

Day 5: A/B Testing + Retrieval Tuning

ðŸŽ¯ Goal: Scientifically prove that the compiled program from Day 4 is actually better.

âœ… What "Done" Looks Like:

Your backend code can now load and use the compiled_scorer.json.

You run a specific scenario (e.g., a party invitation) twice:

Run A (Uncompiled): You measure the town_score. Let's say you get 0.55.

Run B (Compiled): You use the compiled scorer and measure the town_score again. You should see a clear improvement, for example, 0.75.

You have a clear, data-backed conclusion: "The compiled agents showed a 20% improvement in the town score."

ðŸ¤” Why It Matters: This step validates your project's entire hypothesis. It's the "results" section of your experiment.

By following this guide, you can treat the project as a series of clear, manageable steps. Each day builds directly on the last, and you'll have a concrete way to know if you're on the right track before moving on to the next phase. Good luck