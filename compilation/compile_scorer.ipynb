{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: ScoreImportance Compilation with GEPA\n",
    "\n",
    "**Goal**: Compile ScoreImportance module to improve from 77.5% ±2 accuracy to 85%+  \n",
    "**Optimizer**: GEPA (primary choice) with budget=40 rollouts  \n",
    "**Expected Runtime**: 4-6 hours  \n",
    "\n",
    "## Baseline Performance\n",
    "- ±2 Accuracy: 77.5% (31/40 seeds)\n",
    "- MAE: 1.45\n",
    "- Weakest category: Mundane (17%)\n",
    "- Target: ±2 Accuracy > 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q dspy-ai sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (for file persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure if needed\n",
    "!mkdir -p /content/drive/MyDrive/mini-town/compiled\n",
    "!mkdir -p /content/drive/MyDrive/mini-town/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Files\n",
    "\n",
    "**Required files**:\n",
    "1. `scorer_v1.json` - 40 training seeds from Day 3\n",
    "2. `.env` file with GROQ_API_KEY\n",
    "\n",
    "Upload these to `/content/` or copy from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload files directly\n",
    "from google.colab import files\n",
    "print(\"Upload scorer_v1.json:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Option 2: Copy from Google Drive (uncomment if files are in Drive)\n",
    "# !cp /content/drive/MyDrive/mini-town/seeds/scorer_v1.json /content/\n",
    "\n",
    "# Set API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ_API_KEY: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure DSPy with Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "# Configure Groq LLM\n",
    "lm = dspy.LM(\n",
    "    model=\"groq/llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv('GROQ_API_KEY'),\n",
    "    temperature=0.3,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=lm)\n",
    "print(\"✅ DSPy configured with Groq LLM (llama-3.1-8b-instant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ScoreImportance Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreImportance(dspy.Signature):\n",
    "    \"\"\"Rate how important this observation is for the agent's goals.\n",
    "\n",
    "    Score 1-10 where:\n",
    "    - 1-2: Trivial, background noise (e.g., \"grass is green\")\n",
    "    - 3-4: Mildly interesting but not actionable\n",
    "    - 5-6: Relevant to goals, worth remembering\n",
    "    - 7-8: Directly impacts current plans or goals\n",
    "    - 9-10: Life-changing, urgent, critical to goals\n",
    "    \"\"\"\n",
    "\n",
    "    observation: str = dspy.InputField(desc=\"What the agent observed\")\n",
    "    agent_goal: str = dspy.InputField(desc=\"Agent's current high-level goal\")\n",
    "    agent_personality: str = dspy.InputField(desc=\"Agent's personality traits\")\n",
    "\n",
    "    reasoning: str = dspy.OutputField(desc=\"Brief explanation of score\")\n",
    "    score: int = dspy.OutputField(desc=\"Importance score (1-10)\")\n",
    "\n",
    "print(\"✅ ScoreImportance signature defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load seeds\n",
    "with open('scorer_v1.json', 'r') as f:\n",
    "    seeds_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(seeds_data['seeds'])} seeds\")\n",
    "print(f\"Categories: {seeds_data['categories']}\")\n",
    "\n",
    "# Convert to DSPy examples\n",
    "trainset = []\n",
    "for seed in seeds_data['seeds']:\n",
    "    example = dspy.Example(\n",
    "        observation=seed['observation'],\n",
    "        agent_goal=seed['agent_goal'],\n",
    "        agent_personality=seed['agent_personality'],\n",
    "        score=seed['gold_score'],\n",
    "        category=seed['category'],  # For analysis\n",
    "        seed_id=seed['id']  # For tracking\n",
    "    ).with_inputs(\"observation\", \"agent_goal\", \"agent_personality\")\n",
    "    trainset.append(example)\n",
    "\n",
    "print(f\"✅ Created {len(trainset)} training examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample example:\")\n",
    "print(f\"Observation: {trainset[0].observation}\")\n",
    "print(f\"Goal: {trainset[0].agent_goal}\")\n",
    "print(f\"Personality: {trainset[0].agent_personality}\")\n",
    "print(f\"Gold score: {trainset[0].score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Importance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_metric(example, pred, trace=None):\n",
    "    \"\"\"\n",
    "    Metric for ScoreImportance compilation.\n",
    "    \n",
    "    Success criteria:\n",
    "    - Exact match: 1.0\n",
    "    - Within ±1: 0.8\n",
    "    - Within ±2: 0.5\n",
    "    - Within ±3: 0.2\n",
    "    - Else: 0.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_score = int(pred.score)\n",
    "    except (ValueError, AttributeError):\n",
    "        return 0.0\n",
    "    \n",
    "    # Clamp to 1-10\n",
    "    pred_score = max(1, min(10, pred_score))\n",
    "    gold_score = int(example.score)\n",
    "    \n",
    "    error = abs(pred_score - gold_score)\n",
    "    \n",
    "    if error == 0:\n",
    "        return 1.0\n",
    "    elif error <= 1:\n",
    "        return 0.8\n",
    "    elif error <= 2:\n",
    "        return 0.5\n",
    "    elif error <= 3:\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print(\"✅ Importance metric defined\")\n",
    "\n",
    "# Test metric\n",
    "class DummyPred:\n",
    "    def __init__(self, score):\n",
    "        self.score = score\n",
    "\n",
    "example = dspy.Example(score=7)\n",
    "print(f\"Test: gold=7, pred=7 → {importance_metric(example, DummyPred(7)):.2f} (expect 1.0)\")\n",
    "print(f\"Test: gold=7, pred=8 → {importance_metric(example, DummyPred(8)):.2f} (expect 0.8)\")\n",
    "print(f\"Test: gold=7, pred=9 → {importance_metric(example, DummyPred(9)):.2f} (expect 0.5)\")\n",
    "print(f\"Test: gold=7, pred=1 → {importance_metric(example, DummyPred(1)):.2f} (expect 0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Uncompiled Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompiled baseline (ChainOfThought)\n",
    "uncompiled_scorer = dspy.ChainOfThought(ScoreImportance)\n",
    "\n",
    "print(\"✅ Uncompiled baseline created\")\n",
    "print(\"Module type:\", type(uncompiled_scorer).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Uncompiled Baseline\n",
    "\n",
    "Quick check to verify baseline matches Day 3 results (77.5% ±2 accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_module(module, testset, verbose=False):\n",
    "    \"\"\"Evaluate module on test set.\"\"\"\n",
    "    results = {\n",
    "        'exact': 0,\n",
    "        'within_1': 0,\n",
    "        'within_2': 0,\n",
    "        'errors': [],\n",
    "        'predictions': []\n",
    "    }\n",
    "    \n",
    "    for i, example in enumerate(testset):\n",
    "        try:\n",
    "            pred = module(\n",
    "                observation=example.observation,\n",
    "                agent_goal=example.agent_goal,\n",
    "                agent_personality=example.agent_personality\n",
    "            )\n",
    "            pred_score = int(pred.score)\n",
    "            pred_score = max(1, min(10, pred_score))  # Clamp\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error on example {i}: {e}\")\n",
    "            pred_score = 5  # Default\n",
    "        \n",
    "        gold_score = int(example.score)\n",
    "        error = abs(pred_score - gold_score)\n",
    "        \n",
    "        results['errors'].append(error)\n",
    "        results['predictions'].append(pred_score)\n",
    "        \n",
    "        if error == 0:\n",
    "            results['exact'] += 1\n",
    "        if error <= 1:\n",
    "            results['within_1'] += 1\n",
    "        if error <= 2:\n",
    "            results['within_2'] += 1\n",
    "    \n",
    "    n = len(testset)\n",
    "    results['accuracy_exact'] = results['exact'] / n * 100\n",
    "    results['accuracy_within_1'] = results['within_1'] / n * 100\n",
    "    results['accuracy_within_2'] = results['within_2'] / n * 100\n",
    "    results['mean_error'] = sum(results['errors']) / len(results['errors'])\n",
    "    results['max_error'] = max(results['errors'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluating uncompiled baseline (this may take 2-3 minutes)...\\n\")\n",
    "uncompiled_results = evaluate_module(uncompiled_scorer, trainset, verbose=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UNCOMPILED BASELINE PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Exact matches:      {uncompiled_results['exact']:2d}/40 ({uncompiled_results['accuracy_exact']:.1f}%)\")\n",
    "print(f\"Within ±1:          {uncompiled_results['within_1']:2d}/40 ({uncompiled_results['accuracy_within_1']:.1f}%)\")\n",
    "print(f\"Within ±2:          {uncompiled_results['within_2']:2d}/40 ({uncompiled_results['accuracy_within_2']:.1f}%)\")\n",
    "print(f\"Mean Absolute Error: {uncompiled_results['mean_error']:.2f}\")\n",
    "print(f\"Max Error:          {uncompiled_results['max_error']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize GEPA Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.optimizers import GEPA\n",
    "import time\n",
    "\n",
    "# GEPA configuration\n",
    "optimizer = GEPA(\n",
    "    metric=importance_metric,\n",
    "    budget=40,  # 40 rollouts (faster than MIPROv2's 50-100+)\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✅ GEPA optimizer initialized\")\n",
    "print(f\"Budget: 40 rollouts\")\n",
    "print(f\"Expected runtime: 4-6 hours\")\n",
    "print(f\"Checkpoints will be saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GEPA Compilation\n",
    "\n",
    "⚠️ **This cell will take 4-6 hours to run**  \n",
    "✅ You can leave the tab open or use a keep-alive script  \n",
    "💾 Checkpoints saved every 10 iterations to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STARTING GEPA COMPILATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training set size: {len(trainset)}\")\n",
    "print(f\"Budget: 40 rollouts\")\n",
    "print(f\"Expected runtime: 4-6 hours\")\n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n⏳ Compilation running... (this will take a while)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run compilation\n",
    "try:\n",
    "    compiled_scorer = optimizer.compile(\n",
    "        uncompiled_scorer,\n",
    "        trainset=trainset\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ COMPILATION COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Time elapsed: {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n",
    "    print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Compilation failed: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Check Groq API key is valid\")\n",
    "    print(\"2. Check internet connection\")\n",
    "    print(\"3. Try reducing budget to 30\")\n",
    "    print(\"4. Try MIPROv2 optimizer as fallback\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Compiled Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating compiled module (this may take 2-3 minutes)...\\n\")\n",
    "compiled_results = evaluate_module(compiled_scorer, trainset, verbose=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPILED MODULE PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Exact matches:      {compiled_results['exact']:2d}/40 ({compiled_results['accuracy_exact']:.1f}%)\")\n",
    "print(f\"Within ±1:          {compiled_results['within_1']:2d}/40 ({compiled_results['accuracy_within_1']:.1f}%)\")\n",
    "print(f\"Within ±2:          {compiled_results['within_2']:2d}/40 ({compiled_results['accuracy_within_2']:.1f}%)\")\n",
    "print(f\"Mean Absolute Error: {compiled_results['mean_error']:.2f}\")\n",
    "print(f\"Max Error:          {compiled_results['max_error']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Uncompiled vs Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "improvement = compiled_results['accuracy_within_2'] - uncompiled_results['accuracy_within_2']\n",
    "mae_improvement = uncompiled_results['mean_error'] - compiled_results['mean_error']\n",
    "\n",
    "print(\"\\n| Metric | Uncompiled | Compiled | Improvement |\")\n",
    "print(\"|--------|------------|----------|-------------|\")\n",
    "print(f\"| Exact  | {uncompiled_results['accuracy_exact']:5.1f}%   | {compiled_results['accuracy_exact']:5.1f}% | {compiled_results['accuracy_exact'] - uncompiled_results['accuracy_exact']:+6.1f}% |\")\n",
    "print(f\"| ±1     | {uncompiled_results['accuracy_within_1']:5.1f}%   | {compiled_results['accuracy_within_1']:5.1f}% | {compiled_results['accuracy_within_1'] - uncompiled_results['accuracy_within_1']:+6.1f}% |\")\n",
    "print(f\"| **±2** | **{uncompiled_results['accuracy_within_2']:5.1f}%** | **{compiled_results['accuracy_within_2']:5.1f}%** | **{improvement:+6.1f}%** |\")\n",
    "print(f\"| MAE    | {uncompiled_results['mean_error']:5.2f}    | {compiled_results['mean_error']:5.2f}  | {mae_improvement:+6.2f}   |\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Success criteria check\n",
    "if improvement >= 10:\n",
    "    print(\"\\n✅ SUCCESS! Improvement ≥10%, proceed to Day 5\")\n",
    "    print(f\"   Target: 80% → Achieved: {compiled_results['accuracy_within_2']:.1f}%\")\n",
    "elif improvement >= 5:\n",
    "    print(\"\\n⚠️  PARTIAL SUCCESS. Improvement 5-10%, consider iteration\")\n",
    "    print(f\"   Target: 80% → Achieved: {compiled_results['accuracy_within_2']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\n❌ INSUFFICIENT IMPROVEMENT (<5%)\")\n",
    "    print(\"\\n   Options:\")\n",
    "    print(\"   1. Try MIPROv2 optimizer\")\n",
    "    print(\"   2. Add more mundane category seeds\")\n",
    "    print(\"   3. Adjust metric tolerances\")\n",
    "    print(\"   4. Review worst-performing seeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_by_category(module, testset):\n",
    "    \"\"\"Break down performance by category.\"\"\"\n",
    "    category_results = defaultdict(lambda: {'errors': [], 'gold_scores': [], 'pred_scores': []})\n",
    "    \n",
    "    for example in testset:\n",
    "        try:\n",
    "            pred = module(\n",
    "                observation=example.observation,\n",
    "                agent_goal=example.agent_goal,\n",
    "                agent_personality=example.agent_personality\n",
    "            )\n",
    "            pred_score = max(1, min(10, int(pred.score)))\n",
    "        except Exception:\n",
    "            pred_score = 5\n",
    "        \n",
    "        gold_score = int(example.score)\n",
    "        error = abs(pred_score - gold_score)\n",
    "        \n",
    "        cat = example.category\n",
    "        category_results[cat]['errors'].append(error)\n",
    "        category_results[cat]['gold_scores'].append(gold_score)\n",
    "        category_results[cat]['pred_scores'].append(pred_score)\n",
    "    \n",
    "    return category_results\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 UNCOMPILED MODULE BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "uncompiled_by_cat = evaluate_by_category(uncompiled_scorer, trainset)\n",
    "for cat in sorted(uncompiled_by_cat.keys()):\n",
    "    data = uncompiled_by_cat[cat]\n",
    "    within_2 = sum(1 for e in data['errors'] if e <= 2)\n",
    "    accuracy = within_2 / len(data['errors']) * 100\n",
    "    mae = sum(data['errors']) / len(data['errors'])\n",
    "    print(f\"{cat:20s}: {accuracy:5.1f}% ±2 accuracy, MAE={mae:.2f} ({len(data['errors'])} seeds)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 COMPILED MODULE BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compiled_by_cat = evaluate_by_category(compiled_scorer, trainset)\n",
    "for cat in sorted(compiled_by_cat.keys()):\n",
    "    data = compiled_by_cat[cat]\n",
    "    within_2 = sum(1 for e in data['errors'] if e <= 2)\n",
    "    accuracy = within_2 / len(data['errors']) * 100\n",
    "    mae = sum(data['errors']) / len(data['errors'])\n",
    "    \n",
    "    # Get improvement\n",
    "    uncompiled_data = uncompiled_by_cat[cat]\n",
    "    uncompiled_within_2 = sum(1 for e in uncompiled_data['errors'] if e <= 2)\n",
    "    uncompiled_accuracy = uncompiled_within_2 / len(uncompiled_data['errors']) * 100\n",
    "    improvement = accuracy - uncompiled_accuracy\n",
    "    \n",
    "    print(f\"{cat:20s}: {accuracy:5.1f}% ±2 accuracy, MAE={mae:.2f} ({len(data['errors'])} seeds) [{improvement:+5.1f}%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Errors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find worst predictions\n",
    "error_details = []\n",
    "for i, example in enumerate(trainset):\n",
    "    error = compiled_results['errors'][i]\n",
    "    pred_score = compiled_results['predictions'][i]\n",
    "    gold_score = int(example.score)\n",
    "    \n",
    "    error_details.append({\n",
    "        'seed_id': example.seed_id,\n",
    "        'category': example.category,\n",
    "        'observation': example.observation[:60] + '...',\n",
    "        'gold': gold_score,\n",
    "        'pred': pred_score,\n",
    "        'error': error\n",
    "    })\n",
    "\n",
    "# Sort by error descending\n",
    "error_details.sort(key=lambda x: x['error'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔍 TOP 10 LARGEST ERRORS (COMPILED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, err in enumerate(error_details[:10], 1):\n",
    "    print(f\"\\n{i}. Seed #{err['seed_id']}: Error = {err['error']}\")\n",
    "    print(f\"   Observation: \\\"{err['observation']}\\\"\")\n",
    "    print(f\"   Gold: {err['gold']}, Predicted: {err['pred']}\")\n",
    "    print(f\"   Category: {err['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Compiled Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "save_path = '/content/drive/MyDrive/mini-town/compiled/compiled_scorer.json'\n",
    "compiled_scorer.save(save_path)\n",
    "print(f\"✅ Compiled scorer saved to: {save_path}\")\n",
    "\n",
    "# Extract and save prompts for inspection\n",
    "prompt_text = str(compiled_scorer.dump_state())\n",
    "prompt_path = '/content/drive/MyDrive/mini-town/compiled/prompt_scorer.txt'\n",
    "with open(prompt_path, 'w') as f:\n",
    "    f.write(prompt_text)\n",
    "print(f\"✅ Prompts saved to: {prompt_path}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'compilation_time_hours': elapsed / 3600,\n",
    "    'uncompiled': {\n",
    "        'accuracy_within_2': uncompiled_results['accuracy_within_2'],\n",
    "        'mean_error': uncompiled_results['mean_error'],\n",
    "        'exact_matches': uncompiled_results['exact']\n",
    "    },\n",
    "    'compiled': {\n",
    "        'accuracy_within_2': compiled_results['accuracy_within_2'],\n",
    "        'mean_error': compiled_results['mean_error'],\n",
    "        'exact_matches': compiled_results['exact']\n",
    "    },\n",
    "    'improvement': {\n",
    "        'accuracy_delta': improvement,\n",
    "        'mae_delta': mae_improvement\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = '/content/drive/MyDrive/mini-town/compiled/compilation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"✅ Results summary saved to: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎉 COMPILATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download compiled_scorer.json to local project\")\n",
    "print(\"2. Review prompt_scorer.txt to understand optimizations\")\n",
    "print(\"3. Proceed to Day 5: A/B testing in full simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback: MIPROv2 (if GEPA doesn't work)\n",
    "\n",
    "Uncomment and run this cell if GEPA has issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dspy.optimizers import MIPROv2\n",
    "# \n",
    "# print(\"Falling back to MIPROv2 optimizer...\")\n",
    "# \n",
    "# mipro_optimizer = MIPROv2(\n",
    "#     metric=importance_metric,\n",
    "#     auto=\"medium\",\n",
    "#     num_trials=10,\n",
    "#     max_bootstrapped_demos=4,\n",
    "#     max_labeled_demos=5\n",
    "# )\n",
    "# \n",
    "# print(\"Running MIPROv2 compilation (expect 6-8 hours)...\")\n",
    "# start_time = time.time()\n",
    "# \n",
    "# compiled_scorer_mipro = mipro_optimizer.compile(\n",
    "#     uncompiled_scorer,\n",
    "#     trainset=trainset\n",
    "# )\n",
    "# \n",
    "# elapsed = time.time() - start_time\n",
    "# print(f\"\\n✅ MIPROv2 compilation complete! Time: {elapsed/3600:.2f} hours\")\n",
    "# \n",
    "# # Evaluate MIPROv2 results\n",
    "# mipro_results = evaluate_module(compiled_scorer_mipro, trainset)\n",
    "# print(f\"MIPROv2 ±2 accuracy: {mipro_results['accuracy_within_2']:.1f}%\")\n",
    "# print(f\"Improvement: +{mipro_results['accuracy_within_2'] - uncompiled_results['accuracy_within_2']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
