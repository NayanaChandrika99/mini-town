{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dALjZwQKlTmX"
   },
   "source": [
    "# Day 4: ScoreImportance Compilation with GEPA\n",
    "\n",
    "**Goal**: Compile ScoreImportance module to improve from 77.5% ±2 accuracy to 85%+  \n",
    "**Optimizer**: GEPA (primary choice) with budget=40 rollouts  \n",
    "**Expected Runtime**: 4-6 hours  \n",
    "\n",
    "## Baseline Performance\n",
    "- ±2 Accuracy: 77.5% (31/40 seeds)\n",
    "- MAE: 1.45\n",
    "- Weakest category: Mundane (17%)\n",
    "- Target: ±2 Accuracy > 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGK1OhbmlTmZ"
   },
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbWvzrzQlTma",
    "outputId": "ca73cd29-b0ae-4f5a-819d-6edbafa0fa14"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q dspy-ai sentence-transformers accelerate\n",
    "!pip install --quiet gepa==0.0.7  # run once per Colab session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqayoL30lTma"
   },
   "source": [
    "## Mount Google Drive (for file persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKLT7Ir8lTmb",
    "outputId": "bd250948-0a55-4d0b-b0ec-5ddf1b84ea73"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm2jpVq0lTmb"
   },
   "source": [
    "## Upload Files\n",
    "\n",
    "**Required files**:\n",
    "1. `scorer_v1.json` - 40 training seeds from Day 3\n",
    "2. `.env` file with GROQ_API_KEY\n",
    "\n",
    "Upload these to `/content/` or copy from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-bSMy8alTmb",
    "outputId": "2dc46122-6a96-4fbe-b7bf-041e4dcc8437"
   },
   "outputs": [],
   "source": [
    "# Mount/upload your latest JSONL splits first (e.g. via Drive or files tab).\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/mini-town/datasets\")  # adjust if you put them elsewhere\n",
    "train_jsonl = DATA_DIR / \"town_agent_train.jsonl\"\n",
    "dev_jsonl = DATA_DIR / \"town_agent_dev.jsonl\"\n",
    "test_jsonl = DATA_DIR / \"town_agent_test.jsonl\"\n",
    "\n",
    "for path in (train_jsonl, dev_jsonl, test_jsonl):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "\n",
    "print(\"✅ Dataset files located\")\n",
    "print(f\"  Train: {train_jsonl}\")\n",
    "print(f\"  Dev:   {dev_jsonl}\")\n",
    "print(f\"  Test:  {test_jsonl}\")\n",
    "\n",
    "# Set API key (if TOGETHER_API_KEY already set, can skip)\n",
    "import os\n",
    "from getpass import getpass\n",
    "if not os.getenv(\"TOGETHER_API_KEY\"):\n",
    "    together_key = getpass(\"Enter your Together.ai API key: \")\n",
    "    os.environ[\"TOGETHER_API_KEY\"] = together_key\n",
    "    print(\"✅ Together.ai API key set\")\n",
    "else:\n",
    "    print(\"✅ Together.ai API key already set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMjSi_27v4Q-",
    "outputId": "e9bceb6e-2f46-49ec-910d-35b3d6c8b9f1"
   },
   "outputs": [],
   "source": [
    "# API key already handled in cell 6\n",
    "print(\"✅ API key configuration complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x417yz9ilTmb"
   },
   "source": [
    "## Configure DSPy with Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRc4SzQplTmc",
    "outputId": "2f8d294d-c4d7-481c-e506-ccd6aa2d3343"
   },
   "outputs": [],
   "source": [
    "# Configure Together.ai LM\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=\"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    api_key=os.getenv(\"TOGETHER_API_KEY\"),\n",
    "    temperature=0.3,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "reflection_lm = dspy.LM(\n",
    "    model=\"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    api_key=os.getenv(\"TOGETHER_API_KEY\"),\n",
    "    temperature=0.5,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# Re-configure DSPy with Together.ai\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "print(\"✅ DSPy configured with Together.ai\")\n",
    "print(\"   Model: Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
    "\n",
    "# Import GEPA standalone components\n",
    "from gepa.api import optimize as gepa_optimize\n",
    "from gepa.adapters.dspy_adapter.dspy_adapter import DspyAdapter, ScoreWithFeedback\n",
    "from gepa.logging.logger import StdOutLogger\n",
    "\n",
    "print(\"✅ GEPA standalone components imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "577HoiMElTmc"
   },
   "source": [
    "## Define ScoreImportance Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftx6ERaplTmc",
    "outputId": "0c2b2e2e-d895-4a26-82e0-6081e60ca210"
   },
   "outputs": [],
   "source": [
    "class ScoreImportance(dspy.Signature):\n",
    "    \"\"\"Rate how important this observation is for the agent's goals.\n",
    "\n",
    "    Score 1-10 where:\n",
    "    - 1-2: Trivial, background noise (e.g., \"grass is green\")\n",
    "    - 3-4: Mildly interesting but not actionable\n",
    "    - 5-6: Relevant to goals, worth remembering\n",
    "    - 7-8: Directly impacts current plans or goals\n",
    "    - 9-10: Life-changing, urgent, critical to goals\n",
    "    \"\"\"\n",
    "\n",
    "    observation: str = dspy.InputField(desc=\"What the agent observed\")\n",
    "    agent_goal: str = dspy.InputField(desc=\"Agent's current high-level goal\")\n",
    "    agent_personality: str = dspy.InputField(desc=\"Agent's personality traits\")\n",
    "\n",
    "    reasoning: str = dspy.OutputField(desc=\"Brief explanation of score\")\n",
    "    score: int = dspy.OutputField(desc=\"Importance score (1-10)\")\n",
    "\n",
    "print(\"✅ ScoreImportance signature defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MqZzlk3lTmc"
   },
   "source": [
    "## Load and Prepare Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OCCobeclTmd",
    "outputId": "36764508-c28a-4bbf-b207-74f326e93d88"
   },
   "outputs": [],
   "source": [
    "def load_examples(jsonl_path):\n",
    "    \"\"\"Load DSPy examples from JSONL dataset.\"\"\"\n",
    "    examples = []\n",
    "    with Path(jsonl_path).open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            record = json.loads(line)\n",
    "            examples.append(\n",
    "                dspy.Example(\n",
    "                    observation=record.get(\"observation\", record.get(\"recent_observations\", \"\")),\n",
    "                    agent_goal=record.get(\"agent_goal\", \"\"),\n",
    "                    agent_personality=record.get(\"agent_personality\", \"\"),\n",
    "                    score=record.get(\"score\", record.get(\"importance_score\", 5)),\n",
    "                    category=record.get(\"category\", \"unknown\"),\n",
    "                    seed_id=record.get(\"agent_id\", record.get(\"id\", 0)),\n",
    "                ).with_inputs(\"observation\", \"agent_goal\", \"agent_personality\")\n",
    "            )\n",
    "    return examples\n",
    "\n",
    "# Load datasets from JSONL files\n",
    "trainset = load_examples(train_jsonl)\n",
    "devset = load_examples(dev_jsonl)\n",
    "testset = load_examples(test_jsonl)\n",
    "\n",
    "print(f\"✅ Train examples: {len(trainset)}\")\n",
    "print(f\"✅ Dev examples:   {len(devset)}\")\n",
    "print(f\"✅ Test examples:  {len(testset)}\")\n",
    "\n",
    "# Show sample from training set\n",
    "if trainset:\n",
    "    print(\"\\nSample train example:\")\n",
    "    print(f\"  Observation: {trainset[0].observation[:80]}…\")\n",
    "    print(f\"  Goal: {trainset[0].agent_goal}\")\n",
    "    print(f\"  Personality: {trainset[0].agent_personality}\")\n",
    "    print(f\"  Score: {trainset[0].score}\")\n",
    "    print(f\"  Category: {trainset[0].category}\")\n",
    "else:\n",
    "    print(\"⚠️  No training examples loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj1D7OhAlTmd"
   },
   "source": [
    "## Define Importance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPaw04-1lTmd",
    "outputId": "424200cf-e5f8-4ca5-fa59-c4d17d1816e3"
   },
   "outputs": [],
   "source": [
    "def importance_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    Metric for ScoreImportance compilation (GEPA-compatible).\n",
    "\n",
    "    GEPA requires 5 arguments:\n",
    "    - gold: The gold/example object with .score attribute\n",
    "    - pred: The prediction object with .score attribute\n",
    "    - trace: Execution trace (optional, for debugging)\n",
    "    - pred_name: Name of the predictor (optional)\n",
    "    - pred_trace: Prediction trace (optional)\n",
    "\n",
    "    Returns:\n",
    "    - float: Score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract scores from objects\n",
    "        if hasattr(gold, 'score'):\n",
    "            gold_score = int(gold.score)\n",
    "        else:\n",
    "            gold_score = int(gold)\n",
    "\n",
    "        if hasattr(pred, 'score'):\n",
    "            pred_score = int(pred.score)\n",
    "        else:\n",
    "            pred_score = int(pred)\n",
    "    except (ValueError, AttributeError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "    # Clamp predicted score to 1–10\n",
    "    pred_score = max(1, min(10, pred_score))\n",
    "\n",
    "    # Calculate error\n",
    "    error = abs(pred_score - gold_score)\n",
    "\n",
    "    # Return score based on error (higher is better)\n",
    "    if error == 0:\n",
    "        return 1.0      # Exact match\n",
    "    elif error <= 1:\n",
    "        return 0.8      # Within ±1\n",
    "    elif error <= 2:\n",
    "        return 0.5      # Within ±2\n",
    "    elif error <= 3:\n",
    "        return 0.2      # Within ±3\n",
    "    else:\n",
    "        return 0.0      # Poor prediction\n",
    "\n",
    "\n",
    "print(\"✅ GEPA-compatible importance metric defined\")\n",
    "\n",
    "\n",
    "# Dummy test classes\n",
    "class DummyExample:\n",
    "    def __init__(self, score):\n",
    "        self.score = score\n",
    "\n",
    "\n",
    "class DummyPred:\n",
    "    def __init__(self, score):\n",
    "        self.score = score\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_gold = DummyExample(7)\n",
    "print(f\"Test: gold=7, pred=7 → {importance_metric(test_gold, DummyPred(7)):.2f} (expect 1.0)\")\n",
    "print(f\"Test: gold=7, pred=8 → {importance_metric(test_gold, DummyPred(8)):.2f} (expect 0.8)\")\n",
    "print(f\"Test: gold=7, pred=9 → {importance_metric(test_gold, DummyPred(9)):.2f} (expect 0.5)\")\n",
    "print(f\"Test: gold=7, pred=1 → {importance_metric(test_gold, DummyPred(1)):.2f} (expect 0.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDCJFtLalTmd"
   },
   "source": [
    "## Create Uncompiled Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNvYxQgMlTmd",
    "outputId": "50dbfafd-54e3-4fe5-94c7-b154967b338f"
   },
   "outputs": [],
   "source": [
    "# Uncompiled baseline (ChainOfThought)\n",
    "uncompiled_scorer = dspy.ChainOfThought(ScoreImportance)\n",
    "\n",
    "print(\"✅ Uncompiled baseline created\")\n",
    "print(\"Module type:\", type(uncompiled_scorer).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVzWT5JlTmd"
   },
   "source": [
    "## Evaluate Uncompiled Baseline\n",
    "\n",
    "Quick check to verify baseline matches Day 3 results (77.5% ±2 accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9zgLqZSlTmd",
    "outputId": "3313cac6-1942-4a29-9c2c-4e39aa90bb2b"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating uncompiled baseline (this may take 2-3 minutes)...\\n\")\n",
    "uncompiled_results = evaluate_module(uncompiled_scorer, trainset, verbose=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UNCOMPILED BASELINE PERFORMANCE (TRAIN SET)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Exact matches:      {uncompiled_results['exact']:2d}/{len(trainset)} ({uncompiled_results['accuracy_exact']:.1f}%)\")\n",
    "print(f\"Within ±1:          {uncompiled_results['within_1']:2d}/{len(trainset)} ({uncompiled_results['accuracy_within_1']:.1f}%)\")\n",
    "print(f\"Within ±2:          {uncompiled_results['within_2']:2d}/{len(trainset)} ({uncompiled_results['accuracy_within_2']:.1f}%)\")\n",
    "print(f\"Mean Absolute Error: {uncompiled_results['mean_error']:.2f}\")\n",
    "print(f\"Max Error:          {uncompiled_results['max_error']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optional: Evaluate on dev set if available\n",
    "if devset:\n",
    "    print(\"\\nEvaluating on dev set...\\n\")\n",
    "    dev_results = evaluate_module(uncompiled_scorer, devset, verbose=False)\n",
    "    print(f\"Dev set ±2 accuracy: {dev_results['accuracy_within_2']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nnZ1RfklTme"
   },
   "source": [
    "## Initialize GEPA Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeKAhxWTlTme",
    "outputId": "40ecbed9-a3cc-47cf-9b5b-ce2957730255"
   },
   "outputs": [],
   "source": [
    "# Build adapter and feedback map before compilation\n",
    "seed_candidate = {}\n",
    "for name, predictor in uncompiled_scorer.named_predictors():\n",
    "    instructions = getattr(getattr(predictor, \"signature\", None), \"instructions\", None)\n",
    "    if isinstance(instructions, str):\n",
    "        seed_candidate[name] = instructions\n",
    "\n",
    "if not seed_candidate:\n",
    "    raise RuntimeError(\"No predictor instructions found for GEPA seed candidate.\")\n",
    "\n",
    "def score_feedback(component_name: str):\n",
    "    def _fn(predictor_output, predictor_inputs, module_inputs, module_outputs, captured_trace):\n",
    "        score = importance_metric(module_inputs, module_outputs)\n",
    "        feedback_bits = [\n",
    "            f\"[{component_name}] current score: {score:.3f}\",\n",
    "            \"Keep scores aligned with invitations and avoid big mistakes.\",\n",
    "        ]\n",
    "        return ScoreWithFeedback(score=score, feedback=\" \".join(feedback_bits))\n",
    "    return _fn\n",
    "\n",
    "feedback_map = {name: score_feedback(name) for name in seed_candidate}\n",
    "\n",
    "adapter = DspyAdapter(\n",
    "    student_module=uncompiled_scorer,\n",
    "    metric_fn=importance_metric,\n",
    "    feedback_map=feedback_map,\n",
    "    failure_score=0.0,\n",
    "    num_threads=None,\n",
    "    add_format_failure_as_feedback=True,\n",
    ")\n",
    "\n",
    "print(\"✅ GEPA adapter and feedback map configured\")\n",
    "print(f\"Found {len(seed_candidate)} predictor(s) to optimize\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3GQIB21lTme"
   },
   "source": [
    "## Run GEPA Compilation\n",
    "\n",
    "**This cell will take 4-6 hours to run**  \n",
    "You can leave the tab open or use a keep-alive script  \n",
    "Checkpoints saved every 10 iterations to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDehSiMClTme",
    "outputId": "1a8afd0c-d462-470e-9fb1-a7593e667f13"
   },
   "outputs": [],
   "source": "# Reuse existing LM as reflection model\nimport time\n\nlm = dspy.settings.lm\nif lm is None:\n    raise RuntimeError(\"DSPy LM not configured; call configure_dspy() first.\")\n\ndef reflection_lm_fn(prompt: str) -> str:\n    response = lm(prompt)\n    if isinstance(response, str):\n        return response\n    if hasattr(response, \"text\"):\n        return response.text\n    if getattr(response, \"choices\", None):\n        choice = response.choices[0]\n        return getattr(choice, \"text\", None) or choice.get(\"message\", {}).get(\"content\", \"\")\n    return str(response)\n\n# Calculate budget\nmax_metric_calls = max(40 * max(1, len(trainset)), len(trainset))  # keep 40 \"rollouts\"\n\nprint(\"=\" * 70)\nprint(\"STARTING GEPA COMPILATION\")\nprint(\"=\" * 70)\nprint(f\"Training set size: {len(trainset)}\")\nprint(f\"Budget: {max_metric_calls} metric calls\")\nprint(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 70)\nprint(\"\\n Compilation running... (this will take a while)\\n\")\n\nstart_time = time.time()\n\n# Run GEPA compilation\ntry:\n    gepar = gepa_optimize(\n        seed_candidate=seed_candidate,\n        trainset=trainset,\n        valset=trainset,\n        adapter=adapter,\n        reflection_lm=reflection_lm_fn,\n        max_metric_calls=max_metric_calls,\n        logger=StdOutLogger(),\n        display_progress_bar=True,\n        reflection_minibatch_size=5,\n        skip_perfect_score=True,\n    )\n\n    elapsed = time.time() - start_time\n    print(\"\\n\" + \"=\" * 70)\n    print(\"COMPILATION COMPLETE!\")\n    print(\"=\" * 70)\n    print(f\"Time elapsed: {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n    print(f\"Total metric calls: {gepar.total_metric_calls}\")\n    print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(\"=\" * 70)\n\nexcept Exception as e:\n    print(f\"\\n Compilation failed: {e}\")\n    print(\"\\nTroubleshooting steps:\")\n    print(\"1. Check GEPA package installation\")\n    print(\"2. Check internet connection\")\n    print(\"3. Try reducing budget to 30\")\n    print(\"4. Check seed_candidate structure\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7NjgrBllTme"
   },
   "source": [
    "## Materialize Compiled Module\n",
    "\n",
    "# Materialize the compiled module from GEPA's best candidate\n",
    "compiled_scorer = uncompiled_scorer.deepcopy()\n",
    "best_candidate = gepar.best_candidate\n",
    "\n",
    "for name, predictor in compiled_scorer.named_predictors():\n",
    "    if name in best_candidate:\n",
    "        predictor.signature = predictor.signature.with_instructions(best_candidate[name])\n",
    "\n",
    "print(\"✅ Compiled module materialized from GEPA best candidate\")\n",
    "print(f\"Updated {len(best_candidate)} predictor(s)\")\n",
    "\n",
    "# Show the optimized instructions\n",
    "print(\"\\nOptimized instructions:\")\n",
    "for name, instructions in best_candidate.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {instructions[:100]}{'...' if len(instructions) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSERida2lTme",
    "outputId": "c6143ed6-6486-4982-85af-dc7ff94340df"
   },
   "outputs": [],
   "source": [
    "## Evaluate Compiled Module\n",
    "\n",
    "print(\"Evaluating compiled module (this may take 2-3 minutes)...\\n\")\n",
    "compiled_results = evaluate_module(compiled_scorer, trainset, verbose=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPILED MODULE PERFORMANCE (TRAIN SET)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Exact matches:      {compiled_results['exact']:2d}/{len(trainset)} ({compiled_results['accuracy_exact']:.1f}%)\")\n",
    "print(f\"Within ±1:          {compiled_results['within_1']:2d}/{len(trainset)} ({compiled_results['accuracy_within_1']:.1f}%)\")\n",
    "print(f\"Within ±2:          {compiled_results['within_2']:2d}/{len(trainset)} ({compiled_results['accuracy_within_2']:.1f}%)\")\n",
    "print(f\"Mean Absolute Error: {compiled_results['mean_error']:.2f}\")\n",
    "print(f\"Max Error:          {compiled_results['max_error']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optional: Evaluate on dev and test sets if available\n",
    "if devset:\n",
    "    print(\"\\nEvaluating on dev set...\\n\")\n",
    "    dev_compiled_results = evaluate_module(compiled_scorer, devset, verbose=False)\n",
    "    print(f\"Dev set ±2 accuracy: {dev_compiled_results['accuracy_within_2']:.1f}%\")\n",
    "\n",
    "if testset:\n",
    "    print(\"\\nEvaluating on test set...\\n\")\n",
    "    test_compiled_results = evaluate_module(compiled_scorer, testset, verbose=False)\n",
    "    print(f\"Test set ±2 accuracy: {test_compiled_results['accuracy_within_2']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgI5O5E-lTme"
   },
   "source": [
    "## Comparison: Uncompiled vs Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-48XnX9lTme",
    "outputId": "d414f159-920d-4eb1-81a0-8b32f56f4b9c"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "improvement = compiled_results['accuracy_within_2'] - uncompiled_results['accuracy_within_2']\n",
    "mae_improvement = uncompiled_results['mean_error'] - compiled_results['mean_error']\n",
    "\n",
    "print(\"\\n| Metric | Uncompiled | Compiled | Improvement |\")\n",
    "print(\"|--------|------------|----------|-------------|\")\n",
    "print(f\"| Exact  | {uncompiled_results['accuracy_exact']:5.1f}%   | {compiled_results['accuracy_exact']:5.1f}% | {compiled_results['accuracy_exact'] - uncompiled_results['accuracy_exact']:+6.1f}% |\")\n",
    "print(f\"| ±1     | {uncompiled_results['accuracy_within_1']:5.1f}%   | {compiled_results['accuracy_within_1']:5.1f}% | {compiled_results['accuracy_within_1'] - uncompiled_results['accuracy_within_1']:+6.1f}% |\")\n",
    "print(f\"| **±2** | **{uncompiled_results['accuracy_within_2']:5.1f}%** | **{compiled_results['accuracy_within_2']:5.1f}%** | **{improvement:+6.1f}%** |\")\n",
    "print(f\"| MAE    | {uncompiled_results['mean_error']:5.2f}    | {compiled_results['mean_error']:5.2f}  | {mae_improvement:+6.2f}   |\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Success criteria check\n",
    "if improvement >= 10:\n",
    "    print(\"\\n SUCCESS! Improvement ≥10%, proceed to Day 5\")\n",
    "    print(f\"   Target: 80% → Achieved: {compiled_results['accuracy_within_2']:.1f}%\")\n",
    "elif improvement >= 5:\n",
    "    print(\"\\n  PARTIAL SUCCESS. Improvement 5-10%, consider iteration\")\n",
    "    print(f\"   Target: 80% → Achieved: {compiled_results['accuracy_within_2']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\n INSUFFICIENT IMPROVEMENT (<5%)\")\n",
    "    print(\"\\n   Options:\")\n",
    "    print(\"   1. Try MIPROv2 optimizer\")\n",
    "    print(\"   2. Add more mundane category seeds\")\n",
    "    print(\"   3. Adjust metric tolerances\")\n",
    "    print(\"   4. Review worst-performing seeds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQb-ON2DlTme"
   },
   "source": [
    "## Analyze by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeJfNMG6lTme",
    "outputId": "0c162d9a-29d7-4df3-eca5-92088749774c"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_by_category(module, testset):\n",
    "    \"\"\"Break down performance by category.\"\"\"\n",
    "    category_results = defaultdict(lambda: {'errors': [], 'gold_scores': [], 'pred_scores': []})\n",
    "\n",
    "    for example in testset:\n",
    "        try:\n",
    "            pred = module(\n",
    "                observation=example.observation,\n",
    "                agent_goal=example.agent_goal,\n",
    "                agent_personality=example.agent_personality\n",
    "            )\n",
    "            pred_score = max(1, min(10, int(pred.score)))\n",
    "        except Exception:\n",
    "            pred_score = 5\n",
    "\n",
    "        gold_score = int(example.score)\n",
    "        error = abs(pred_score - gold_score)\n",
    "\n",
    "        cat = example.category\n",
    "        category_results[cat]['errors'].append(error)\n",
    "        category_results[cat]['gold_scores'].append(gold_score)\n",
    "        category_results[cat]['pred_scores'].append(pred_score)\n",
    "\n",
    "    return category_results\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" UNCOMPILED MODULE BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "uncompiled_by_cat = evaluate_by_category(uncompiled_scorer, trainset)\n",
    "for cat in sorted(uncompiled_by_cat.keys()):\n",
    "    data = uncompiled_by_cat[cat]\n",
    "    within_2 = sum(1 for e in data['errors'] if e <= 2)\n",
    "    accuracy = within_2 / len(data['errors']) * 100\n",
    "    mae = sum(data['errors']) / len(data['errors'])\n",
    "    print(f\"{cat:20s}: {accuracy:5.1f}% ±2 accuracy, MAE={mae:.2f} ({len(data['errors'])} examples)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" COMPILED MODULE BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compiled_by_cat = evaluate_by_category(compiled_scorer, trainset)\n",
    "for cat in sorted(compiled_by_cat.keys()):\n",
    "    data = compiled_by_cat[cat]\n",
    "    within_2 = sum(1 for e in data['errors'] if e <= 2)\n",
    "    accuracy = within_2 / len(data['errors']) * 100\n",
    "    mae = sum(data['errors']) / len(data['errors'])\n",
    "\n",
    "    # Get improvement\n",
    "    uncompiled_data = uncompiled_by_cat[cat]\n",
    "    uncompiled_within_2 = sum(1 for e in uncompiled_data['errors'] if e <= 2)\n",
    "    uncompiled_accuracy = uncompiled_within_2 / len(uncompiled_data['errors']) * 100\n",
    "    improvement = accuracy - uncompiled_accuracy\n",
    "\n",
    "    print(f\"{cat:20s}: {accuracy:5.1f}% ±2 accuracy, MAE={mae:.2f} ({len(data['errors'])} examples) [{improvement:+5.1f}%]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5pp2BqilTme"
   },
   "source": [
    "## Top Errors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UvREaXklTme",
    "outputId": "9b908a0a-7bc2-403d-8be5-8321e94c9e63"
   },
   "outputs": [],
   "source": [
    "# Find worst predictions\n",
    "error_details = []\n",
    "for i, example in enumerate(trainset):\n",
    "    error = compiled_results['errors'][i]\n",
    "    pred_score = compiled_results['predictions'][i]\n",
    "    gold_score = int(example.score)\n",
    "\n",
    "    error_details.append({\n",
    "        'seed_id': example.seed_id,\n",
    "        'category': example.category,\n",
    "        'observation': example.observation[:60] + '...',\n",
    "        'gold': gold_score,\n",
    "        'pred': pred_score,\n",
    "        'error': error\n",
    "    })\n",
    "\n",
    "# Sort by error descending\n",
    "error_details.sort(key=lambda x: x['error'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" TOP 10 LARGEST ERRORS (COMPILED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, err in enumerate(error_details[:10], 1):\n",
    "    print(f\"\\n{i}. Example #{err['seed_id']}: Error = {err['error']}\")\n",
    "    print(f\"   Category: {err['category']}\")\n",
    "    print(f\"   Observation: \\\"{err['observation']}\\\"\")\n",
    "    print(f\"   Gold: {err['gold']}, Predicted: {err['pred']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfqRBlg9lTmf"
   },
   "source": [
    "## Save Compiled Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kaym4Ye1lTmf",
    "outputId": "e168e97e-c294-4cd9-affa-31d8fa684822"
   },
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "save_path = '/content/drive/MyDrive/mini-town/compiled/compiled_scorer.json'\n",
    "compiled_scorer.save(save_path)\n",
    "print(f\" Compiled scorer saved to: {save_path}\")\n",
    "\n",
    "# Extract and save prompts for inspection\n",
    "prompt_text = str(compiled_scorer.dump_state())\n",
    "prompt_path = '/content/drive/MyDrive/mini-town/compiled/prompt_scorer.txt'\n",
    "with open(prompt_path, 'w') as f:\n",
    "    f.write(prompt_text)\n",
    "print(f\" Prompts saved to: {prompt_path}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'compilation_time_hours': elapsed / 3600,\n",
    "    'uncompiled': {\n",
    "        'accuracy_within_2': uncompiled_results['accuracy_within_2'],\n",
    "        'mean_error': uncompiled_results['mean_error'],\n",
    "        'exact_matches': uncompiled_results['exact']\n",
    "    },\n",
    "    'compiled': {\n",
    "        'accuracy_within_2': compiled_results['accuracy_within_2'],\n",
    "        'mean_error': compiled_results['mean_error'],\n",
    "        'exact_matches': compiled_results['exact']\n",
    "    },\n",
    "    'improvement': {\n",
    "        'accuracy_delta': improvement,\n",
    "        'mae_delta': mae_improvement\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = '/content/drive/MyDrive/mini-town/compiled/compilation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\" Results summary saved to: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" COMPILATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download compiled_scorer.json to local project\")\n",
    "print(\"2. Review prompt_scorer.txt to understand optimizations\")\n",
    "print(\"3. Proceed to Day 5: A/B testing in full simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amdWfHJElTmf"
   },
   "source": [
    "## Fallback: MIPROv2 (if GEPA doesn't work)\n",
    "\n",
    "Uncomment and run this cell if GEPA has issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kY6pgdClTmf"
   },
   "outputs": [],
   "source": [
    "# from dspy.optimizers import MIPROv2\n",
    "#\n",
    "# print(\"Falling back to MIPROv2 optimizer...\")\n",
    "#\n",
    "# mipro_optimizer = MIPROv2(\n",
    "#     metric=importance_metric,\n",
    "#     auto=\"medium\",\n",
    "#     num_trials=10,\n",
    "#     max_bootstrapped_demos=4,\n",
    "#     max_labeled_demos=5\n",
    "# )\n",
    "#\n",
    "# print(\"Running MIPROv2 compilation (expect 6-8 hours)...\")\n",
    "# start_time = time.time()\n",
    "#\n",
    "# compiled_scorer_mipro = mipro_optimizer.compile(\n",
    "#     uncompiled_scorer,\n",
    "#     trainset=trainset\n",
    "# )\n",
    "#\n",
    "# elapsed = time.time() - start_time\n",
    "# print(f\"\\n MIPROv2 compilation complete! Time: {elapsed/3600:.2f} hours\")\n",
    "#\n",
    "# # Evaluate MIPROv2 results\n",
    "# mipro_results = evaluate_module(compiled_scorer_mipro, trainset)\n",
    "# print(f\"MIPROv2 ±2 accuracy: {mipro_results['accuracy_within_2']:.1f}%\")\n",
    "# print(f\"Improvement: +{mipro_results['accuracy_within_2'] - uncompiled_results['accuracy_within_2']:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}